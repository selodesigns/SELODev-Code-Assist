# Local LLM Configuration (PRIMARY - FREE)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=codellama:7b-instruct
# Alternative models: codellama:13b-instruct, deepseek-coder:6.7b, starcoder2:7b

LM_STUDIO_BASE_URL=http://localhost:1234/v1
LM_STUDIO_MODEL=local-model

# Local Model Settings
LOCAL_MODEL_TIMEOUT=30
LOCAL_MODEL_MAX_TOKENS=512
LOCAL_MODEL_TEMPERATURE=0.1

# Cloud API Keys (OPTIONAL FALLBACK - COSTS MONEY)
# Only uncomment if you want expensive cloud fallback
# OPENAI_API_KEY=your_openai_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Application Configuration
FLASK_ENV=development
FLASK_DEBUG=True
API_PORT=5000
API_HOST=0.0.0.0

# Code Analysis Settings
MAX_FILE_SIZE_MB=10
SUPPORTED_LANGUAGES=python,javascript,typescript,html,css,json,yaml,go,rust,java,cpp
DEFAULT_AI_PROVIDER=ollama
# Options: ollama, lm_studio, openai, anthropic

# Security
SECRET_KEY=your_secret_key_here
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
